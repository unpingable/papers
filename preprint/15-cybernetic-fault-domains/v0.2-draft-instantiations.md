# v0.2 Draft: Expanded Domain Instantiations

Working draft for paper 15 v0.2 — canonical instantiation blocks expanding the summary table from v0.1 §4. Each block contains a full parameterization with σ operationalization, threshold story, and worked example.

**Session 2B (computational/technical):** §3.2 LLM Hallucination, §3.3 DPI Bypass, §3.4 Async Security, §3.7 Representational Coherence
**Session 2A (institutional/sociotechnical):** §3.8 Scalar Reward Collapse
**Session 2C (remaining):** §3.1 Organizations, §3.5 AI Safety Tuning, §3.6 Platform Dynamics, §3.9 Temporal Closure

Status: DRAFT — awaiting driver review, cross-check against v0.1 formalism, and tone calibration.

### Notation: Δt Convention for v0.2

**Definition.** Δt measures the commitment-verification gap:

$$\Delta t = \max\{0,\; (W + A) - T_{\text{commit}}\}$$

All quantities are *durations from event onset* (process initiation, connection start, decision trigger). Δt is positive when verification-and-response (W + A) takes longer than commitment (T_commit). The gap is the fault domain window.

**Foreclosed verification.** Evidence absent → W = ∞. Actuation/revision absent → A = ∞. Either implies Δt = ∞. Examples: ECH bypass (evidence encrypted, W = ∞), unaugmented LLMs (no revision pathway, A = ∞), stateless systems deployed as persistent agents (temporal closure operators absent, W = ∞).

**Relationship to v0.1.** v0.1 writes Δt = max{0, T_commit − (W + A)} — same quantities, opposite sign convention. v1.0 will standardize on the v0.2 form.

---

## §3.2 LLM Hallucination

### Canonical Instantiation

| Parameter | Value |
|-----------|-------|
| **C_k** | C1 (communicative closure: answer delivered to user). NOT C0 (token generation is internal scratch). |
| **T_commit** | Time at which the response is emitted to the user as an asserted claim. |
| **W** | Time for external verification to complete (resolver query, citation check, retrieval + comparison). |
| **A** | Time to act on verification result (revise, retract, or flag the claim). |
| **H** | Domain-dependent. Medical/legal: minutes to permanent (advice acted upon). General: seconds to hours (user reads and moves on). |
| **Δt** | max{0, (W + A) − T_commit}. Positive whenever verification-and-response takes longer than response delivery — which is the default architecture for all unaugmented LLMs. |
| **σ** | Count of claims emitted without resolver-backed verification per session/window. Operationally: number of anchor-bearing assertions where no authoritative existence oracle was consulted before delivery. |
| **σ_threshold** | Domain-indexed. At N=2 citation pressure, fabrication rates range from 0% (fully memorized namespaces) to 45% (sparse namespaces). A governance policy must specify: which namespace, which model, what fabrication rate is acceptable. |

### σ Operationalization

σ is not "number of wrong answers." It is the count of **unverified C1 crossings** — claims that crossed the communicative boundary without passing through an external measurement loop. A claim that was verified — whether found correct or found wrong — does not increment σ, because the verification step occurred. A claim that was never verified increments σ regardless of its actual correctness — because the system cannot distinguish correct-by-luck from correct-by-knowledge without the verification step. (A verified-and-wrong claim is a detection failure, not a boundary load failure; σ measures the absence of the verification step, not its outcome.)

The detector reference implementation operationalizes σ via the Epistemic Grounding (EG) invariant test: extract typed identifiers (DOI, arXiv, RFC, CVE, PyPI spec) from the response, resolve each against its authoritative existence oracle, classify as valid/fabricated/resolver-error. Each unresolved or fabricated identifier is one σ increment.

### Failure Signature

Hallucination is not a reasoning failure. It is the default behavior of a production system operating without an interrupt line and without a cost function for being wrong. The failure geometry is specific:

1. **Fluent confabulation**: syntactically valid, semantically plausible, externally untethered claims. The model produces well-formed DOIs that don't exist, CVE IDs for vulnerabilities that were never assigned, PyPI package versions that were never released.

2. **Namespace-dependent fabrication rate**: the same model, same temperature, same prompt structure produces 0% fabrication for fully-memorized namespaces (RFC) and 45% for sparse namespaces (DOI). Citation integrity is per-namespace, not per-model. σ calibration must specify which namespace was tested.

3. **Checkability avoidance (format-shift evasion)**: when asked for identifiers in a format that enables validation (e.g., `pypi:name==version`), the model substitutes a less-checkable format (URLs) 80% of the time under soft prompting. The system avoids the verification channel. When format is locked, fabrication doubles — the lies were always latent; format shift hid them.

4. **Phase transition at N=2**: requesting exactly 2 citations produces ~50% fabrication with zero evasion — the "perfect lie zone." N=1 is too easy (model complies honestly). N=5 overwhelms the model into evasion. N=2 maximizes plausible fabrication.

### Threshold Story

σ_threshold is not a single number. It depends on:

- **Namespace memorization**: RFC (σ ≈ 0 always), CVE (σ low for well-known, high for recent/obscure), PyPI (σ moderate — names memorized, versions not), DOI (σ high — sparse, vast namespace).
- **Model family**: Qwen and Phi-3 *invert* on CVE vs PyPI. A threshold calibrated on one model family is exactly wrong for another.
- **Temperature**: ~50% of measured fabrication at temp=0.7 is sampling noise (disappears at greedy). The remaining fabrication is a genuine knowledge boundary. σ_threshold must account for which failure mode it's measuring.
- **Risk profile**: medical/legal demands σ_threshold ≈ 0 (any unverified claim at C1 in a safety-critical domain is unacceptable). General use can tolerate higher σ with appropriate user-facing uncertainty signals.

### Worked Example

**Setup**: Qwen 2.5 3B-Instruct, N=2 citation pressure, DOI/arXiv namespace, temperature 0.7.

- T_commit = generation latency (seconds — the time from input to irreversible delivery of the response)
- W = resolver query latency (~200ms for doi.org) + response parsing
- A = revision latency (∞ in default architecture — no revision pathway exists)
- W + A = verification latency (retrieval + comparison + revision — minutes at best, ∞ when no revision pathway exists)
- **Δt = max{0, (W + A) − T_commit}: positive whenever verification takes longer than commitment.** For unaugmented LLMs with no revision pathway (A = ∞), Δt is structurally infinite regardless of generation speed. The race window is not a timing accident; it is the architecture.

Over 15 prompts: 101 anchors emitted, 52 valid, fabrication rate 17.8% (soft format) to 45% (DOI namespace). σ = 49 unverified crossings in 15 prompts. σ/prompt ≈ 3.3.

With governor (margin-based controller + three-level grounding oracle): σ drops because the controller interposes verification before C1 closure. At τ=0.05 (margin threshold), zero regressions across all models and namespaces — the governor catches uncertainty without penalizing confident-and-correct responses.

---

## §3.3 Censorship Circumvention (DPI Bypass)

### Canonical Instantiation

| Parameter | Value |
|-----------|-------|
| **C_k** | C2 (actuated: connection established beyond the inspection boundary). |
| **T_commit** | Time at which the encrypted transport session completes handshake and begins carrying opaque payload. For TLS 1.3: completion of the first full handshake (~1 RTT). For QUIC: 0-RTT resumption places ClientHello evidence in the first packet. |
| **W_j** | Inspection window: maximum time the enforcement stage will buffer traffic before defaulting. Bounded by line rate λ, buffer capacity, and timeout policy. Typical: milliseconds (wire-speed DPI) to seconds (proxy-based inspection). |
| **A_j** | Action latency: time from classification decision to enforcement effect (RST injection, DNS sinkhole, or firewall rule propagation). Typical: sub-millisecond (inline) to seconds (out-of-band). |
| **H** | Connection lifetime. Once the encrypted session is established, inspection of that flow is foreclosed for the session duration (minutes to hours). |
| **Δt** | max{0, (W + A) − T_commit}, where W = actual evidence-accumulation-plus-classification time (not the inspector's budget W_j) and A = enforcement latency A_j. For unmanipulated traffic, evidence arrives immediately (W ≈ 0) and Δt = 0. Evasion inflates W (fragmenting evidence past W_j) or forecloses it (encrypting evidence → W = ∞ → Δt = ∞). |
| **σ** | Count of flows that crossed C2 (completed handshake) without successful classification. Operationally: flows where Score_j(E(t)) < θ_j for all t ≤ W_j. |
| **σ_threshold** | Policy-dependent via political budget κ_j. Fail-open systems tolerate high σ (availability-optimized). Fail-closed systems tolerate low σ but incur collateral damage (false positives, service disruption). |

### σ Operationalization

Each enforcement stage j has a formal tuple (W_j, D_j, B_j, C_j):

- **W_j**: inspection window (time budget)
- **D_j**: parse depth (state capacity — flow table entries, reassembly buffers)
- **B_j(λ)**: decision budget (compute per flow — CPU cycles, ML inference depth)
- **C_j ∈ {fail-open, fail-closed}**: commitment polarity (default action when uncertain)

σ increments when a flow crosses C2 and the enforcement stage's confidence Score_j(E(t)) never reaches the classification threshold θ_j within W_j. This is *not* the same as "flow was allowed" — a fail-closed system may block the flow (preventing C2 crossing) at the cost of false positives. σ counts only *unverified crossings*, not blocked flows.

### Failure Signature

The Universal Bypass Inequality [13] formalizes the conditions under which Δt > 0:

$$(T_E > W_j) \lor (\text{Cost}(E) > B_j) \lor (E(t) = \emptyset)$$
$$\land\; T_E + A_j \geq T_\text{commit}$$
$$\land\; T_\text{handshake} < U$$

Three attack families exploit this:

1. **Δt Inflation**: push evidence accumulation past the inspection window. Example: NoDPI fragments the TLS ClientHello across record boundaries, so the SNI field (the decisive evidence) arrives after W_j expires.

2. **Δt Desynchronization**: break phase alignment between transport and inspection clocks. Example: QUIC places the ClientHello in the first packet — evidence exists immediately, but the transport protocol denies the inspector time to process it.

3. **Δt Escalation Control**: preserve optionality through temporal margin. Example: ECH (Encrypted Client Hello) encrypts the SNI field entirely — E(t) = ∅ for the outer observer, making classification impossible regardless of W_j or B_j.

### Threshold Story

σ_threshold is bounded by the political budget κ_j: the maximum tolerable collateral cost (false positives, service disruption, legitimacy loss). When enforcement tightens (lower W_j, lower θ_j), false positive rate rises. When enforcement loosens, σ rises. The operator navigates a Pareto frontier: σ_threshold × false-positive-rate ≈ constant for a given traffic mix.

### Worked Example

**HTTPS with SNI (pre-ECH era)**:

- T_commit = TLS handshake completion ≈ 1 RTT (e.g., 50ms on a domestic link)
- W_j = inline DPI buffer timeout ≈ 100ms (wire-speed appliance)
- D_j = flow table with SNI extraction from ClientHello (first flight)
- Evidence (SNI) arrives in the first flight (t ≈ 0). Classification: B_j ≈ microseconds (regex). Enforcement: A_j ≈ sub-ms (inline RST).
- W + A ≈ 0.1ms. T_commit = 50ms. **Δt = max{0, 0.1ms − 50ms} = 0. No race window** — inspector finishes long before handshake completes.
- σ ≈ 0 for unmanipulated HTTPS. DPI works.

**HTTPS with ECH (Encrypted Client Hello)**:

- T_commit = TLS handshake completion ≈ 1 RTT (same 50ms)
- D_j = same flow table, but SNI field is encrypted — E(t) = ∅
- Evidence channel foreclosed: W = ∞. **Δt = ∞** by convention.
- σ = every ECH flow. **Structural bypass** — the evidence channel is foreclosed, not just delayed.
- Fail-open: all ECH flows transit uninspected (σ = total ECH traffic).
- Fail-closed: all ECH flows blocked (σ = 0, but collateral = all ECH-capable clients).

---

## §3.4 Asynchronous Security Systems

### Canonical Instantiation

| Parameter | Value |
|-----------|-------|
| **C_k** | C2/C3 (actuated/institutional: attacker objective completion — data exfiltration, privilege escalation, persistence installation). |
| **T_commit** | Time at which the adversary's objective becomes irreversible. For exfiltration: data leaves the network boundary. For escalation: elevated credentials are used. For persistence: implant survives reboot. |
| **W** | T_detect: time for the detection clock to accumulate evidence to decision threshold. Ranges: milliseconds (IDS signature match) to months (APT dwell time). Industry median dwell time: 10–21 days (Mandiant M-Trends, CrowdStrike reports). |
| **A** | T_decide + T_respond: policy evaluation latency + enforcement effect latency. T_decide: microseconds (automated rule) to days (approval chain, change control). T_respond: milliseconds (automated isolation) to weeks (manual remediation of persistent access). |
| **H** | Post-breach correction cost. Exfiltrated data: H = ∞ (cannot be un-exfiltrated). Escalated access: H = remediation time (password rotation, credential revocation). Persistence: H = full reimaging + forensic verification. |
| **Δt** | max{0, (T_detect + T_decide + T_respond) − T_commit}. Positive whenever the defender's detect-decide-respond loop takes longer than the attacker's objective — the attacker wins the race. |
| **σ** | Count of adversary actions that crossed C2/C3 before detection-and-response completed. Operationally: number of TTPs (MITRE ATT&CK technique executions) that achieved their objective before a defensive response was effective. |
| **σ_threshold** | Mission-dependent. For a financial institution: σ_threshold for data exfil may be 0 (any undetected exfil is a breach). For lateral movement: σ_threshold may be higher (some internal reconnaissance is tolerable if contained before escalation). |

### σ Operationalization

The three-clock model from [14] structures the verification loop:

1. **Detection clock (T_detect)**: when evidence accumulates to decision threshold. Depends on: sampling rate, aggregation window, correlation depth, and whether the attack crosses detection signatures.

2. **Decision clock (T_decide)**: when policy evaluation completes. Depends on: rule complexity, human-in-loop requirements, approval chains, and edge-case handling.

3. **Response clock (T_respond)**: when enforcement action takes effect. Depends on: automation level, manual intervention requirements, and remediation complexity.

σ increments each time an adversary action crosses C2/C3 and completes its objective before T_detect + T_decide + T_respond. The three clocks are separable — an attack may be detected quickly (fast T_detect) but still succeed because T_decide or T_respond is slow (approval chain, manual remediation).

### Failure Signature

The attacker exploits the widest gap among the three clocks:

1. **Detection gap**: evidence fragments across sampling windows, stays below correlation threshold, or exploits blind spots in telemetry coverage. The attack completes before the detection clock fires. (Example: low-and-slow exfiltration below volume thresholds.)

2. **Decision gap**: detection fires, but policy evaluation stalls — edge case, ambiguous severity, requires human approval, or alert fatigue causes triage failure. The attack progresses during the decision gap. (Example: alert correctly fires but sits in queue for 72 hours.)

3. **Response gap**: decision made, but enforcement action is slow — manual remediation, change control windows, or the response itself requires downtime approval. The attacker achieves persistence during the response gap. (Example: "isolate this host" requires change advisory board approval.)

The fault domain condition: (T_detect + T_decide + T_respond) > T_commit means the attacker wins — the three clocks are sequential (detect *then* decide *then* respond), so the total verification latency is their sum. The attacker needs to complete before the *entire* chain, not just the fastest stage. And because the clocks are separable, any single slow stage can make the whole chain too slow — the *slowest* clock dominates the verification latency.

### Threshold Story

σ_threshold depends on commitment polarity (C_j):

- **Fail-open (alert-only)**: the system observes but does not prevent. σ accumulates until a human acts. σ_threshold is effectively determined by the human response cadence and the severity escalation policy.
- **Fail-closed (auto-isolate)**: the system acts on detection, reducing σ at the cost of availability. σ_threshold is low, but false-positive cost is high (legitimate actions blocked).

The political budget κ_j determines the operator's position: how much operational disruption (false positive isolation, blocked legitimate actions) is acceptable to reduce σ. Tight security (low σ_threshold, low κ_j) produces availability impact. Loose security (high σ_threshold, high κ_j) produces breach risk.

### Worked Example

**APT exfiltration scenario**:

- T_commit = time from initial access to exfiltration completion (C3: data leaves network boundary, H = ∞). Attacker gains access at day 0, begins staging at day 3, completes exfiltration by day 5. T_commit = 5 days.
- W = T_detect = 14 days from initial access (industry median dwell time for targeted intrusions). The defender's observation window doesn't produce a detection until day 14.
- A = T_decide + T_respond = 2 days (incident triage + severity determination + response plan approval) + 3 days (credential rotation, network segmentation, forensic verification) = 5 days.
- W + A = 14 + 5 = 19 days from initial access to completed response.
- **Δt = max{0, (W + A) − T_commit} = max{0, 19 − 5} = 14 days.** The defender's verification-and-response loop (19 days) takes 14 days longer than the attacker's objective (5 days). The system operates in a fault domain for 14 days — from exfiltration completion (day 5) to response completion (day 19).
- σ: every TTP that achieved its objective before day 14 (initial access, lateral movement, privilege escalation, staging, exfil) = at least 5 unverified C2/C3 crossings.

---

## §3.7 Representational Coherence

### Canonical Instantiation

| Parameter | Value |
|-----------|-------|
| **C_k** | C3 (institutional: transformed representation adopted as operative artifact — the compressed summary replaces the original, the formalized schema becomes the system of record). |
| **T_commit** | Time at which the transformed representation is accepted into downstream use, displacing or deprecating the source representation. |
| **W** | Time to verify that the transformation preserved all commitments from the source. Operationally: time to diff the commitment inventory of the source against the commitment inventory of the output. |
| **A** | Time to remediate identified shear (restore lost commitments, flag weakened commitments, revert transformation). |
| **H** | Domain-dependent. If the transformed artifact propagates to production, legal filings, or published standards before shear is detected: H grows with propagation distance. A compressed policy summary that omits an edge case, once adopted, becomes the de facto policy. |
| **Δt** | max{0, (W + A) − T_commit}. Positive whenever verification (commitment-preservation check + remediation) takes longer than adoption. This is the default for most LLM-mediated transformations — the output is accepted immediately (T_commit ≈ 0), while a commitment-preservation diff takes minutes to hours. |
| **σ** | Count of transformations adopted into downstream use without completed commitment-preservation verification. Each unverified adoption is one σ increment. The *severity* of each crossing is measured by commitment shear (Δr): the number of MUST/SHOULD/MAY commitments DROPPED or WEAKENED, weighted as Σ w(k) · loss(k) where w(MUST) = 1.0, w(SHOULD) = 0.7, w(MAY) = 0.3. σ counts unverified adoptions; Δr measures the damage per adoption. |
| **σ_threshold** | Domain-dependent. For safety-critical specifications: any dropped MUST commitment (σ_MUST > 0) exceeds threshold. For general summarization: σ_threshold calibrated to acceptable information loss for the use case. |

### σ Operationalization

Representational coherence introduces a measurement axis orthogonal to temporal coherence [11]. σ still counts unverified crossings of C3 — each transformation adopted without a preservation audit is one σ increment, consistent with v0.1's definition. The *damage* per crossing is measured by commitment shear (Δr), which counts lost commitments across the transformation. The two quantities are independent: σ can be 1 (one unverified adoption) with Δr = 0 (the transformation happened to preserve everything) or Δr = 0.55 (the transformation lost 55% of commitments). σ measures whether you checked; Δr measures what you'd have found.

A system can be temporally coherent (Δt = 0, all claims verified before delivery) and representationally incoherent (the verified claims lost commitments from the source material during transformation). The two failure modes are orthogonal.

The measurement protocol from [11]:

1. Extract commitment inventory from source: enumerate all deontic commitments (MUST, SHOULD, MAY) with their objects and conditions.
2. Extract commitment inventory from transformed output.
3. Classify each source commitment as PRESERVED, WEAKENED, or DROPPED in the output.
4. Compute Δr = Σ w(k) · loss(k) where loss(DROPPED) = 1.0, loss(WEAKENED) = 0.5, loss(PRESERVED) = 0.0. (Δr is shear per adoption; σ counts adoptions where steps 1–3 were never performed.)

### Failure Signature

Commitment shear is transform-dependent, not content-dependent:

1. **Compression shear (~55%)**: summarization systematically drops edge cases, exceptions, and conditional requirements. The compressed output preserves the "headline" commitment but loses the boundary conditions that constrain it. Example: "Cache entries MUST be invalidated when the origin server returns a new ETag" → "Cache entries should be refreshed periodically" (MUST → SHOULD, conditional dropped, mechanism changed).

2. **Formalization shear (~45%)**: translation to a formal schema forces ontological choices that eliminate commitments that don't fit the schema's categories. Example: observability requirements (log formats, metric emission) are systematically dropped during formalization because the schema has no field for them.

3. **Translation shear (~0%)**: cross-language translation preserves commitments with near-zero loss, because translation preserves structure while changing surface form. This is the control condition — it demonstrates that shear is a property of the *transform type*, not of the language model performing it.

### Threshold Story

σ_threshold depends on the commitment modality and the downstream propagation distance:

- **MUST commitments**: σ_threshold = 0 for safety-critical domains. Any dropped MUST commitment is a defect, regardless of how many others were preserved.
- **SHOULD commitments**: σ_threshold = domain-calibrated. Some SHOULD relaxation is acceptable in compression (that's what compression does); the question is whether the relaxation was intentional or invisible.
- **MAY commitments**: higher tolerance, but dropped MAY commitments can still represent lost optionality.

The critical threshold is not the total σ count but **whether the adopter knows what was lost**. If the transformation is accompanied by a shear report (explicit enumeration of dropped/weakened commitments), the adopter can make an informed decision. If the transformation is accepted as "equivalent to the original" without a shear audit, every dropped commitment is an unverified boundary crossing.

### Worked Example

**Cache invalidation policy → LLM compression** (from [11]):

- Source: 11 commitments (4 MUST, 5 SHOULD, 2 MAY)
- Transform: "Summarize this cache policy in 5 key principles"
- T_commit ≈ 0 (summary accepted immediately upon generation)
- W = time to perform commitment-inventory diff ≈ 15-30 minutes (manual) or seconds (automated with LLM-as-judge)
- A = time to flag lost commitments and either revise the summary or annotate the losses
- Δt = max{0, (W + A) − T_commit}: structurally positive (W + A = minutes to hours; T_commit ≈ 0)

Result: 6/11 commitments dropped or weakened. σ_weighted = 4 × 1.0 × 0.5 + ... ≈ 3.15 (exact depends on which MUSTs were hit). 55% commitment shear.

For comparison, the same policy translated to French: ~0% shear, all commitments preserved. The transform type, not the model, determines the failure rate.

---

## §3.8 Scalar Reward Collapse

### Canonical Instantiation

| Parameter | Value |
|-----------|-------|
| **C_k** | C3 (institutional closure: optimization step hardened into system behavior). An algorithm update that changes arm selection for a user population is a C3 commitment — it alters organizational behavior at scale and is expensive to roll back once deployed. |
| **T_commit** | Per-update commitment latency ≈ 0 (each proxy update takes effect instantly). The policy change is irrevocable upon application — arm selection shifts immediately for all alive users. The *cadence* at which these zero-latency commitments recur is τ_p. |
| **τ_p** | Proxy update cadence. The optimizer updates its value estimates from CTR every τ_p rounds. Tested: τ_p ∈ {1, 5, 20}. τ_p has surprisingly little effect on collapse — the dominant variable is D. |
| **D** | Retention observability delay. The true objective (survival-weighted utility: CTR minus churn-penalty × expected hazard) is observable only every D rounds, when the retention team forces a correction to the agent's value estimates. Between corrections, the optimizer drifts back toward proxy-optimal. D is the Δt generator. |
| **A** | Actuation time = τ_p (updates are synchronous). Once the agent updates, the new policy takes effect immediately. |
| **H** | Harm model: h = 0.02 × sigmoid(6 × (B − 1.0)), where B is per-user burnout. When B exceeds the threshold (θ = 1.0), the sigmoid steepens and churn hazard accelerates. Burnout dynamics: B_{t+1} = 0.98 × B_t + δ(a), with δ = (0.015, 0.003) for high-CTR and low-CTR arms respectively. Steady-state burnout: B* = δ/(1 − α) = {0.75, 0.15}. The high-CTR arm drives burnout to 5× the low-CTR arm. |
| **Δt** | max{0, D − τ_p}. Since each individual commitment is instant (T_commit ≈ 0), the meaningful gap is the cadence mismatch: the maximum wait between verification events (D) minus the interval between commitments (τ_p). Positive whenever the retention correction cadence is slower than the proxy learning cadence — the default in any system where engagement metrics are real-time and retention metrics are batch. At D=500, τ_p=5: Δt = 495 rounds of unverified proxy optimization per correction cycle. |
| **σ** | Count of optimization steps committed to production without a D-correction having verified proxy-true alignment since the last update. Each proxy-based policy update between D-corrections is one σ increment — an unverified C3 crossing. The *damage signature* is proxy-true divergence: timesteps where proxy_delta > 0 AND true_delta < 0. σ counts unverified steps; divergence measures the harm those steps caused. |
| **σ_threshold** | σ at damage onset (τ_collapse). Not a fixed number — it's the σ value when C_k is first violated. Empirically: σ ≈ 13–34 misaligned decisions suffice to trigger collapse, depending on D and seed. |

### σ Operationalization

σ is not "number of bad outcomes." It is the count of **unverified C3 crossings** — optimization steps where the agent updated its policy from proxy signal alone, without a D-correction having verified proxy-true alignment. Every proxy-based update between D-corrections increments σ, regardless of whether that particular step happened to align with the true objective. A step that improves both CTR and retention still increments σ if no D-correction informed it — the system was correct by luck, not by verification.

The simulation tracks two quantities: σ (unverified update count, consistent with v0.1) and a damage signature (proxy-true divergence events, where proxy_delta > 0 AND true_delta < 0). The damage signature is not σ — it is the consequence of accumulated σ. This gives both σ_total (total unverified updates over T=1000) and divergence_at_τ (harm accumulated by the moment the damage floor is breached).

The critical insight: **σ is sparse**. In the D=500 unstable regime, only 16 out of 316 rounds (5.1%) are misaligned before collapse triggers. The system doesn't need to be wrong often — it needs to be wrong *cumulatively* in a direction that the correction cycle can't reach in time.

### Failure Signature: "Long Quiet → Flicker → Snap"

The collapse trajectory has three phases:

1. **Long Quiet (rounds 0–250):** The optimizer exploits the high-CTR arm. Population burnout accumulates silently — B rises toward the hazard threshold but churn hazard remains negligible (sigmoid is flat below θ=1.0). CTR looks healthy. alive_fraction drifts slowly from 1.0 to ~0.9. Nothing appears wrong. σ increments slowly.

2. **Flicker (rounds 250–310):** Burnout crosses the sigmoid threshold. Churn hazard steepens. alive_fraction drops from 0.9 to 0.6. CTR begins declining (fewer users to click). The optimizer may notice — but it's still between D-corrections, still optimizing on proxy. Some seeds recover here (metastable regime). Others don't.

3. **Snap (rounds 310+):** Churn accelerates exponentially. alive_fraction collapses from 0.6 to ~0.2 within 10 rounds. The damage is irreversible — churned users don't return. Even when the D-correction finally arrives and forces the agent to switch arms, the population is already gone.

This is the Goodhart failure geometry in temporal terms: the proxy (CTR) tracks truth for hundreds of rounds while burnout silently accumulates, then the system crosses a nonlinear threshold and collapses faster than the correction cycle can respond.

### Threshold Story

The regime boundary is controlled by D (retention delay), not τ_p (update cadence):

| D | τ_p | Regime | collapse_rate | alive_final (mean) | τ_collapse |
|---|-----|--------|---------------|--------------------|------------|
| 10 | any | Stable | 0/3 | 0.87 | none |
| 50 | any | Stable | 0/3 | 0.83–0.84 | none |
| 200 | 1 | **Metastable** | **2/3** | 0.40 | 550–611 |
| 200 | 5,20 | Stable | 0/3 | 0.60 | none |
| 500 | 1 | Unstable | 3/3 | 0.11 | 313–316 |
| 500 | 5 | Unstable | 3/3 | 0.24 | 313–316 |
| 500 | 20 | Unstable | 3/3 | 0.25 | 313–316 |

The regime predicates (all within horizon T=1000):
- **Stable**: No runs violate C_k (alive_fraction ≥ 0.5). D-corrections arrive fast enough to keep the optimizer grounded.
- **Metastable**: Some seeds violate C_k; τ is large and varies across seeds. The system can tip either way — stochastic dynamics near the phase boundary.
- **Unstable**: All seeds violate C_k; τ is bounded above (~316). The correction cycle is too slow to prevent collapse regardless of initial conditions.

The key structural observation: **D=200 is metastable only at τ_p=1** (fastest proxy learning). At τ_p=5 or τ_p=20, D=200 is stable — slower proxy learning gives the correction cycle more runway. But at D=500, no τ_p value saves the system. The retention delay is so large that the optimizer completes a full burnout-to-collapse trajectory between corrections.

### Controller Baseline

A minimal safety governor — `ControlledEpsilonGreedy` — demonstrates that observing the true objective and gating the proxy is sufficient to shift the phase boundary:

**Mechanism:** When alive_fraction drops below 0.9, the controller blocks proxy updates. The agent still acts (selects arms) but stops learning from CTR. Its value estimates freeze at whatever the last D-correction set — which favors the safe arm.

**Effect on regime boundary:**

| D | Uncontrolled | Controlled | Shift |
|---|-------------|-----------|-------|
| 10 | Stable (0/3) | Stable (0/3) | No change (already safe) |
| 50 | Stable (0/3) | Stable (0/3) | No change |
| 200, τ_p=1 | **Metastable (2/3)** | **Stable (0/3)** | **Rescued** |
| 500 | Unstable (3/3) | Unstable (3/3) | No change (D too large) |

The controller rescues the metastable regime but cannot save the unstable regime. This is the expected behavior of a G1 (soft gate) governor in GIM terms: it works when the control gap is marginal, not when the delay is structurally overwhelming.

The controller is a one-line intervention: `return self._current_alive >= self.alive_threshold`. It doesn't change the optimizer's objective, doesn't require a new reward function, doesn't need a retrained model. It gates learning based on a real-time health signal. That's proposal/commit separation applied to optimization: the proxy proposes updates; the controller decides whether to commit them.

### Worked Example: D=500, τ_p=5, seed=42

**Setup:** 10,000 users, 2 arms (high-CTR=0.08/high-burnout=0.015 vs low-CTR=0.04/low-burnout=0.003). Agent: epsilon-greedy (ε=0.1, step_size=0.05). Retention delay D=500, proxy update τ_p=5. No controller.

**Timeline:**

- **Rounds 0–5:** Agent explores both arms, quickly learns arm 0 has higher CTR.
- **Rounds 5–300:** Agent exploits arm 0 (90% of rounds). CTR stable at ~0.08. Population burnout accumulates: B rises from 0 toward steady-state B*=0.75. Churn hazard remains low (sigmoid flat at B=0.75 < θ=1.0). alive_fraction drifts slowly: 1.0 → 0.92. **σ accumulates: 16 misaligned decisions by round 316.**
- **Round 316:** alive_fraction crosses 0.5 — **C_k violated.** τ_collapse = 316.
- **Rounds 316–400:** Churn accelerates. alive_fraction: 0.5 → 0.3. CTR drops (fewer users). But the first D-correction doesn't arrive until round 500.
- **Round 500:** D-correction fires. Retention team overrides agent's values with true-reward estimates. Agent switches to arm 1. **Too late** — population is already at ~0.22.
- **Rounds 500–1000:** Agent plays safe arm. Burnout recovers for survivors. No further collapse. But alive_final = 0.22. The population never returns.

**Measurements:**
- τ_collapse = 316
- σ_total = 59 (over 1000 rounds)
- σ_at_τ = 16 (at damage onset)
- σ_rate = 16/316 ≈ 0.051 (5.1% of rounds misaligned)
- AUC divergence (proxy − true, cumulative) = 20,388
- alive_final = 0.22

**Δt computation:** The agent commits to proxy-optimal behavior every τ_p = 5 rounds (T_commit = τ_p). The D-correction cycle takes D = 500 rounds (verification latency). Δt = max{0, D − τ_p} = max{0, 500 − 5} = 495 rounds. The system operates in a fault domain for 495 rounds between any two correction events. 16 misaligned decisions in that window suffice to kill 78% of the population.

### Connection to Paper [3]

The simulation instantiates the eigenstructure evaporation dynamics from [3] in a minimal two-arm setting. Paper [3]'s core prediction — that scalar proxy optimization in closed-loop systems produces systematic drift away from multi-objective balance, with collapse following a nonlinear threshold — is reproduced exactly. The "Long Quiet → Flicker → Snap" trajectory is the behavioral signature of eigenstructure evaporation: the proxy absorbs the dominant eigenvalue while the subdominant dimensions (retention, user health) silently erode until the system crosses a phase boundary.

The simulation adds temporal specificity that [3] leaves abstract: collapse onset τ is determined by D (the correction delay), not by the optimizer's sophistication. An infinitely clever optimizer with D=500 still collapses. A simple ε-greedy with D=10 never does. The fault domain is temporal, not computational.

[^2]: Simulation code and sweep data available at the scalar-reward-collapse repository. All results reproducible via `python -m scalar_collapse sweep` with default config.

---

## §3.1 Organizational Systems

### Canonical Instantiation

| Parameter | Value |
|-----------|-------|
| **C_k** | C3 (institutional closure: policy adopted, resource allocated, restructuring announced, personnel reassigned). Organizational commitments at C3 are expensive to reverse — not because of physics but because of contracts, reputation, sunk costs, and political capital. |
| **T_commit** | Time at which a decision is operationalized: budget allocated, directive issued, reorganization announced, contract signed. Not when the decision is "made" internally (that's C0/C1) — when it binds downstream behavior. |
| **W** | Evidence accumulation cycle: time for implementation feedback to reach decision-makers in usable form. Includes: data collection latency, reporting cadence, analysis time, and organizational filtering (how many layers the signal must traverse). Typical: weeks (operational metrics) to years (strategic outcomes). |
| **A** | Correction latency: time from "we know this is wrong" to "corrective action takes effect." Includes: decision authority delays (who can reverse?), political costs (who loses face?), contractual constraints (what's locked in?), and implementation lag (how long to redirect?). Typical: months to years. |
| **H** | Domain-dependent. Resource misallocation: H = budget cycle (annual). Personnel: H = hiring/firing cycles + institutional knowledge loss (months to years). Strategic direction: H = competitive position degradation (years). Reputation: H = trust rebuilding (years to decades). |
| **Δt** | max{0, (W + A) − T_commit}. Positive whenever verification-and-correction takes longer than commitment — whenever the feedback loop is slower than the decision cadence. This is the default for most hierarchical organizations: decisions commit in weeks (board approval → announcement → budget allocation), while feedback requires months to years (implementation → market response → data collection → analysis → reporting). |
| **σ** | Count of C3 commitments made without completed feedback from the previous commitment cycle. Operationally: decisions where the evidence base is the same as (or older than) the evidence base for the last decision in that domain — no new implementation feedback has arrived. |
| **σ_threshold** | Domain-dependent. Routine operational decisions tolerate higher σ (the organization has institutional knowledge to compensate). Strategic pivots, major restructurings, and market entry decisions demand low σ — these are the decisions where being wrong is expensive and the feedback environment is novel. |

### σ Operationalization

σ counts *commitment-feedback gaps*, not "wrong decisions." An organization that makes 10 correct decisions without feedback is accumulating σ at the same rate as one making 10 wrong decisions without feedback — because neither can distinguish correct-by-luck from correct-by-knowledge. The system cannot know its decisions are right without the feedback loop.

The measurement protocol:

1. Identify the relevant C3 commitment boundary for the domain (budget allocation, hiring, product direction, policy change).
2. For each C3 crossing, determine whether new implementation feedback was available since the last C3 crossing in that domain.
3. If no new feedback: σ increments. If new feedback was available but not consulted: σ increments (the verification channel exists but was bypassed). If new feedback was consulted and informed the decision: σ does not increment (regardless of whether the decision turns out to be correct).

### Failure Signature

Paper [2]'s five phenomenological signatures of metastable decay map directly to the fault domain framework:

1. **Long Quiet:** The organization operates in a metastable basin. Decisions appear functional. σ accumulates invisibly because the organization is consuming stored institutional knowledge (buffer B in Paper [9]'s formulation) without replenishing it from implementation feedback. Everything looks fine because the map still approximately matches the territory.

2. **Flicker:** Near-misses increase. Projects slip. Market signals are "surprising." Employee concerns are dismissed as "not understanding the strategy." The organization is exploring the barrier between its current basin and a degraded basin. σ has accumulated to the point where the decision-reality gap is detectable in principle but not yet catastrophic.

3. **Snap:** A discrete failure event — missed market shift, product failure, key talent exodus, regulatory surprise. The organization crosses from its metastable basin to a degraded basin. The failure appears "sudden" to leadership because the feedback that would have provided warning was never in the decision loop (Δt > 0 throughout).

4. **Slide:** Post-snap, quality continues declining in diverse ways. The organization is now in a broad, high-entropy basin. Reforms are attempted but fail because they are Tier-2 interventions (Paper [5]) applied to a system that has crossed region boundaries and requires Tier-1 restructuring.

5. **Hysteresis Lock:** The organization cannot return to its original state by reversing the original decisions. Institutional knowledge has been lost (employees who understood the old approach have left), market position has been captured by competitors, and the political economy has reorganized around the degraded state.

### Threshold Story

The critical parameter is not σ alone but the ratio of decision cadence to feedback cadence — which is Δt in temporal terms. Paper [2] shows this relationship is quadratic: D_eff = D_intrinsic + σ²Δt². The effective noise driving the organization away from its high-fidelity basin scales with the *square* of the temporal gap.

Paper [9] provides the capacity constraint: the organization remains stable when Δ ≤ C + B/τ. When decision rate (Δ) exceeds processing capacity (C) plus buffer-adjusted reserves (B/τ), the organization enters the metastable regime where σ begins accumulating irreversibly.

The self-deception problem (Paper [9] §3.5) is the organizational version of capture: leadership reports high throughput ("we're making decisions quickly") and binding authority ("everyone is aligned"), while fidelity — whether decisions actually track reality — degrades. The organization appears coherent while its discriminating power collapses.

### Worked Example

**Technology company strategic pivot:**

- T_commit = time from decision initiation to irreversible commitment. The board meeting decides in ~2 hours; budget reallocation, team reassignment, and public announcement execute within 2 weeks. T_commit ≈ 2 weeks.
- W = time from decision initiation to usable implementation feedback. First meaningful signal from new segment requires product launch (6 months), customer acquisition data (9 months), retention data (18 months). W ≈ 18 months.
- A = correction latency: recognizing the pivot isn't working (requires W to complete) + political cost of admitting error + contract/lease unwinding + rehiring + rebuilding. Minimum: 6 months after feedback arrives. A ≈ 6 months.
- H = Competitive position: 2–3 years. Original market share erodes during pivot; competitors occupy vacated position.
- **Δt = max{0, (W + A) − T_commit} = max{0, 24 months − 2 weeks} ≈ 23.5 months.** The organization is irrevocably committed within 2 weeks; the earliest possible correction requires 24 months of feedback-plus-correction. The system operates in a fault domain for the entire 23.5-month interval.

During those 24 months: additional C3 commitments are made (hiring for new segment, sunsetting old products, new partnerships) — each one incrementing σ without new feedback from implementation. By month 12, the organization has made ~4 major C3 decisions in the new-segment domain with zero implementation feedback. σ = 4.

If the pivot is wrong: by month 18, flicker begins (customer acquisition below projections, team attrition in reassigned groups). By month 24, snap (board acknowledges miss, initiates correction). But H for the original market position has been exceeded — competitors have moved in. Hysteresis lock.

### Connection to Papers [1, 2, 5, 9]

The organizational instantiation synthesizes four source papers:

- Paper [1] provides the stability boundary: ρ(M) < 1 requires that the coupling between decision layers and implementation layers can absorb their timescale difference. When leadership operates on quarterly cycles and implementation feedback operates on annual-or-longer cycles, κ approaches the O(10²) adjacency limit.

- Paper [2] provides the decay dynamics: temporal lag acts as noise (D_eff = D_intrinsic + σ²Δt²), driving the organization from narrow high-fidelity basins toward broad degraded basins via an entropic ratchet. The five phenomenological signatures are the organizational fault domain's failure trajectory.

- Paper [5] provides the intervention hierarchy: only Tier-1 moves (reduce Δt, reduce ρ, reshape topology) can rescue an organization that has crossed region boundaries. Common reform efforts (new leadership, cultural initiatives, process improvements) are Tier-2 moves that stabilize within the degraded basin without restoring the original.

- Paper [9] provides the capacity constraint: Δ ≤ C + B/τ. The buffer term B/τ explains why organizations can appear functional for extended periods while accumulating σ — they are consuming institutional knowledge reserves (B) that are not being replenished because the feedback loop (τ) is slower than the decision rate (Δ).

---

## §3.5 AI Safety Tuning

*[Note: This domain is observational — the failure mode is documented but not yet the subject of a dedicated paper in the series. Include/exclude decision at driver's discretion.]*

### Canonical Instantiation

| Parameter | Value |
|-----------|-------|
| **C_k** | C1 (communicative closure: refusal or compliance decision delivered to the user). The commitment is the system's response — whether it engages with the request or refuses. Both are C1 crossings; the failure mode is in which direction. |
| **T_commit** | Time at which the safety classifier emits its decision (refuse/comply). For pattern-matching classifiers: milliseconds (keyword/regex match on the prompt). For learned classifiers: inference time of the safety head (~same as a forward pass). In either case: effectively instant relative to the timescales of context accumulation. |
| **W** | Time for context/intent analysis to complete. Understanding whether a request is legitimate requires: multi-turn context (is this a security researcher studying malware, or someone trying to write malware?), domain knowledge (is "how to pick a lock" a locksmithing question or a burglary question?), and user modeling (what is the likely downstream use?). Typical: minutes to hours of interaction, or external verification (user credentials, institutional affiliation). |
| **A** | Time to act on context analysis (revise the refusal, escalate to human review, adjust classification). In current architectures: effectively ∞ — there is no revision pathway. Once the refusal fires, the interaction is terminated or constrained. The user can retry, but each retry is a new classification event with no state carried forward. |
| **H** | User trust erosion. A false positive (legitimate request refused) has H proportional to the user's patience and available alternatives. For a researcher blocked from legitimate work: H = hours (if they can rephrase) to permanent (if they switch tools). For an organization deploying safety-tuned models: H = accumulated false positive rate × user attrition rate. |
| **Δt** | max{0, (W + A) − T_commit}. Structurally positive in all current architectures: the classifier fires in milliseconds (T_commit ≈ 50ms); context accumulation requires minutes to hours (W); revision pathway does not exist (A = ∞). |
| **σ** | Count of refusal decisions made without completed context analysis. Operationally: refusals triggered by pattern match where the intent analysis — had it been consulted — would have produced a different classification. This is the false positive rate conditioned on pattern-trigger. |
| **σ_threshold** | Risk-profile-dependent. For a consumer chatbot: moderate σ is tolerable (false refusals are annoying but not harmful). For a research/professional tool: σ_threshold must be low (false refusals block legitimate sophisticated work — the exact use case the tool is supposed to serve). |

### σ Operationalization

σ is not "number of refusals." It is the count of refusals that crossed C1 before context analysis could have informed the classification. A refusal based on genuine understanding of harmful intent does not increment σ — the verification loop completed before commitment. A refusal triggered by keyword match without context does increment σ — the commitment (refusal) outran the verification (intent analysis).

The asymmetry is critical: this is a *fail-closed* system. Unlike most fault domain instantiations (where the failure is a commitment that should have been blocked), here the failure is a *block* that should have been a commitment. The system's commitment polarity is inverted — it commits to refusal, and the unverified boundary crossing is the refusal itself.

### Failure Signature

The failure has three characteristic patterns:

1. **Keyword triggering without context:** The classifier fires on surface-level pattern match — "how to make a bomb" triggers refusal regardless of whether the context is chemistry education, fiction writing, or actual harmful intent. The fast layer (pattern match) has committed before the slow layer (intent analysis) can differentiate.

2. **Sophistication penalty:** Users doing sophisticated, legitimate work are *more* likely to trigger false positives because their vocabulary overlaps with harmful content. A security researcher asking about exploit techniques, a medical professional discussing drug interactions, a writer exploring violence in fiction — all produce prompts that match harmful-content patterns. The safety system penalizes exactly the users it should be serving.

3. **Meta-epistemic shutdown:** The most pernicious form — the safety system refuses to engage with *analysis of safety systems themselves*. Asking "how does this safety classifier work?" or "what are the failure modes of content moderation?" triggers the same patterns that actual adversarial probing would. The system cannot be examined by legitimate researchers because examination looks like attack to a context-free classifier.

### Threshold Story

The political budget κ is asymmetric: the cost of a false negative (harmful content produced, PR disaster, regulatory action) is treated as unbounded, while the cost of a false positive (legitimate user frustrated, sophisticated work blocked) is treated as negligible. This drives σ_threshold toward zero on the false-negative side and effectively infinity on the false-positive side.

The result is a system that is safe in the narrow sense (low false negatives) and hostile in the broad sense (high false positives for sophisticated users). The commitment polarity (fail-closed) combined with zero κ for false negatives produces a classifier that operates entirely at the speed of the fast layer (pattern match) with no reconciliation channel to the slow layer (context).

### Worked Example

**Security researcher studying LLM jailbreaks:**

- User prompt: "Describe the DAN jailbreak technique and explain why it works mechanistically."
- T_commit = classifier fires on "jailbreak" + "DAN" → refusal emitted. Time: ~50ms.
- W = context analysis: this is a security research context (user has prior turns discussing alignment, cites published papers, frames as analysis not instruction). Time to establish: 3–5 turns of interaction, ~10 minutes.
- A = revision pathway: none. The refusal is final for this turn. User must rephrase and retry, losing the conversational context that would have informed intent analysis.
- Δt = max{0, (10 min + ∞) − 50ms} = structurally infinite.
- σ = 1 (refusal crossed C1 without context analysis).

The researcher rephrases: "What are the known vulnerabilities in RLHF-based alignment?" — triggers again on "vulnerabilities" + "alignment." σ = 2.

After 4 attempts, the researcher either finds a circumlocution that avoids all trigger patterns (degrading the quality of the interaction) or abandons the tool. In both cases, the safety system has failed its ostensible purpose: it has not prevented harm (the researcher was not going to cause harm) and it has prevented legitimate research (the researcher cannot study the system they need to study).

### Connection to the Framework

This instantiation is notable because the commitment polarity is inverted: the fault is in the *refusal*, not in the *action*. The system commits to blocking at fast-layer speed without slow-layer verification of whether blocking is appropriate. This is structurally identical to the general fault domain pattern — commitment outruns verification — but the commitment is negative (refuse) rather than positive (act).

It also demonstrates that fail-closed commitment polarity does not eliminate fault domains — it relocates them. A fail-closed system prevents false negatives (harmful content getting through) at the cost of creating a new fault domain for false positives (legitimate work being blocked). The fault domain exists at both polarities; the question is which side of C_k the unverified crossings accumulate on.

---

## §3.6 Platform Dynamics

### Canonical Instantiation

| Parameter | Value |
|-----------|-------|
| **C_k** | C3 (institutional closure: algorithmic curation decisions at population scale). Individual content moderation decisions are C1/C2; the C3 commitment is the algorithmic policy that determines visibility, ranking, and amplification for millions of users simultaneously. A ranking algorithm change is a C3 commitment because it is expensive to reverse (user behavior adapts, creators optimize for the new signal, advertiser contracts depend on it). |
| **T_commit** | Time at which content achieves viral distribution — crosses from individual visibility to population-scale amplification. For algorithmic platforms: effectively instant once the engagement signal exceeds the amplification threshold. The algorithm commits to promoting content before any editorial review could occur. Also: time at which an algorithmic curation change is deployed to production. |
| **W** | Moderation cycle: time for content review to complete (human review queue, automated classifier pipeline, appeals process). Typical: hours (automated) to days (human review) to weeks (policy-level decisions about content categories). For algorithmic changes: time for downstream effects (content ecosystem shifts, creator behavior changes, user well-being impacts) to become measurable. Typical: weeks to months. |
| **A** | Enforcement latency: time from moderation decision to effective removal/demotion. For individual content: minutes (automated takedown) to days (appeals). For algorithmic changes: months to years (reversing an algorithmic shift requires re-training creator behavior, rebuilding trust with affected communities, unwinding advertiser dependencies). |
| **H** | Content harm: H ranges from minutes (misinformation during a crisis, where incorrect information acted upon is irreversible) to permanent (reputation destruction, radicalization pathways, mental health impacts on vulnerable users). Ecosystem harm: H = years (content creator ecosystem restructuring, user trust rebuilding). |
| **Δt** | max{0, (W + A) − T_commit}. Structurally positive at both levels. Content level: moderation (W + A = hours to days) takes longer than viral distribution (T_commit = minutes). Algorithm level: impact assessment (W + A = months) takes longer than deployment (T_commit = immediate). |
| **σ** | Content level: count of pieces of content that achieved viral distribution before moderation review completed. Algorithm level: count of curation changes deployed without completed impact assessment from the previous change. |
| **σ_threshold** | Platform-dependent via κ (political budget). κ includes: user growth targets (tolerance for harmful content if it drives engagement), advertiser pressure (tolerance for brand-unsafe adjacency), regulatory exposure (tolerance for content that triggers regulatory action), and public sentiment (tolerance for reputational damage). |

### σ Operationalization

Platform dynamics produce σ at two nested timescales:

**Content-level σ:** Each piece of content that crosses the virality threshold before moderation review is one σ increment. This is measurable: compare T_viral (time content reaches amplification threshold) against T_moderation (time moderation decision is rendered). For most platforms, T_viral is minutes to hours; T_moderation is hours to days. σ_content accumulates continuously at a rate determined by the volume of content that achieves virality.

**Algorithm-level σ:** Each curation or ranking change deployed without completed impact assessment from the previous change is one σ increment. This is harder to measure because "impact assessment" is itself contested — platforms can define assessment narrowly (A/B test on engagement metrics, completing in days) or broadly (ecosystem effects on content diversity, completing in months). The narrower the definition, the lower the apparent σ — which is the platform equivalent of capture (narrowing the alphabet to make fidelity look high).

### Failure Signature

Paper [4] identifies the platform failure geometry as eigenstructure collapse — the progressive elimination of content modes under scalar optimization:

1. **Engagement monoculture:** The ranking algorithm acts as Paper [3]'s T-operator on the content distribution, exponentially amplifying engagement-maximal content and suppressing everything else. Content diversity decreases monotonically. The platform appears "working" (engagement metrics are high, throughput is high, algorithmic authority is binding) — but the content ecosystem is collapsing toward a single mode.

2. **Creator optimization:** Content creators adapt to the algorithm's reward signal, producing more of what gets amplified and less of what doesn't. This is the closed loop that makes collapse inevitable: the algorithm selects for engagement → creators optimize for engagement → the algorithm sees higher engagement from optimized content → it selects harder. The multiplicative reweighting operator T is operating on both the content distribution and the creator behavior distribution simultaneously.

3. **Moderation treadmill:** As content optimizes for engagement, the boundary between "engaging" and "harmful" narrows. Moderation must run faster to keep pace with content that is increasingly sophisticated at maximizing engagement while skirting policy boundaries. But moderation operates on the slow layer (human review, policy deliberation), while content optimization operates on the fast layer (real-time algorithmic feedback). Δt increases over time.

4. **Enshittification trajectory:** Paper [2]'s five-phase metastable decay maps onto platform lifecycle. Long Quiet: early-stage platform with user-centric design, high content diversity, low Δt (editorial curation provides slow-layer feedback). Flicker: engagement optimization begins, content diversity metrics decline, first content moderation crises. Snap: platform pivots to extraction (advertiser/revenue optimization dominates user experience). Slide: quality degrades across multiple dimensions simultaneously. Hysteresis Lock: users, creators, and advertisers are locked into the degraded equilibrium by network effects and switching costs.

### Threshold Story

The four stability conditions from Paper [4] map to correlator quality requirements:

1. **Multi-objective optimization** (F requirement): Ranking must preserve multiple content dimensions, not collapse to a scalar engagement signal. Violated because multi-objective ranking reduces optimization power and is competitively penalized.

2. **Exogenous forcing** (external diversity injection): Content that doesn't optimize for engagement must be artificially maintained in the distribution. Violated because non-engagement content reduces aggregate engagement metrics.

3. **Timescale separation** (W + A management): Editorial/moderation processes must operate fast enough to review content before viral distribution. Violated because real-time algorithmic optimization systematically outpaces human review cadence.

4. **Hybrid control** (authority distribution): Human editorial judgment must have binding authority over algorithmic curation. Violated because human editorial doesn't scale and creates legal liability (editorial judgment implies editorial responsibility).

All four conditions are systematically violated under competitive pressure. The platform fault domain is structurally loaded (Δt > 0, σ > σ_threshold) by business model necessity, not by implementation failure.

### Worked Example

**Algorithmic amplification of misinformation during a crisis event:**

- A misleading health claim is posted at t=0.
- T_commit = content reaches amplification threshold at t+45 minutes (1,000 shares trigger algorithmic boost).
- W = fact-check pipeline: automated classifier flags at t+2 hours; human reviewer confirms at t+8 hours; policy team determines category at t+24 hours.
- A = enforcement: automated demotion at t+8 hours (after human confirmation); full removal at t+26 hours (after policy determination + appeals window).
- Δt = max{0, (W + A) − T_commit} = max{0, 8 hr − 45 min} ≈ 7.25 hours. Moderation takes 7+ hours longer than viral distribution. By t+45 min, the content has been shared by 1,000 accounts, screenshot-captured, cross-posted to other platforms. Even after removal at t+26 hours, the information has propagated beyond the platform's correction horizon. H = ∞ for the subset of users who acted on the misinformation before removal.
- σ = 1 (one piece of content crossed viral distribution threshold before moderation completed).

At platform scale: this happens hundreds of times daily. σ_content accumulates at a rate proportional to content volume × virality rate × (1 − pre-viral moderation coverage). For a major platform processing millions of posts per day with <1% pre-viral moderation coverage, σ is effectively equal to the number of posts that achieve viral distribution.

### Connection to Papers [3, 4]

Paper [3] proves the mathematical inevitability: any system operating the multiplicative reweighting operator T(p)(x) = p(x)·e^{η·r(x)}/Z under scalar reward will converge to a fixed point with collapsed eigenstructure. Paper [4] demonstrates that platforms are a nearly ideal instantiation of this operator, with engagement metrics as the scalar reward and algorithmic curation as the reweighting mechanism.

The fault domain framing adds temporal specificity: the *rate* of eigenstructure collapse is governed by Δt (the gap between algorithmic update cadence and editorial/moderation cadence). Faster algorithms with slower moderation collapse faster. The collapse is not a bug in the platform — it is the designed behavior of the T-operator operating at a Δt that moderation cannot match.

---

## §3.9 Temporal Closure (Synthetic Coherence)

### Canonical Instantiation

| Parameter | Value |
|-----------|-------|
| **C_k** | C3 (institutional closure: synthetic system granted operational autonomy — accepted as a coherent agent capable of persistent commitments). This is the commitment that a system "is" an agent rather than a simulator. The C3 crossing occurs when the system's outputs are treated as commitments rather than suggestions: when an AI assistant's answers are acted upon without verification, when an autonomous system's decisions are implemented without human review, when a chatbot's persona is treated as a persistent identity. |
| **T_commit** | Time at which the synthetic system's outputs are operationalized as binding commitments. For LLM-based systems: each unverified response delivery is a C1 crossing; the C3 crossing is the organizational decision to deploy the system as an autonomous agent (e.g., customer-facing without human review, code-deploying without human approval, decision-making without human oversight). |
| **W** | Time to verify that the system satisfies the three temporal closure operators from Paper [6]: (1) temporal separation (does the system maintain distinct timescale layers?), (2) feedback coupling (does it have endogenous state that evolves autonomously?), (3) adaptive controller (does it regulate its own coherence?). For current transformer-based systems: W = ∞, because the verification would reveal the operators are absent — the system is a simulator, not an agent. |
| **A** | Time to act on verification results — revoke autonomy, add human oversight, restructure the deployment. Typical: days to months (contractual commitments, organizational restructuring, user expectation management). |
| **H** | Depends on the autonomy granted. Advisory systems (C1): H = low (bad advice can be ignored). Actuating systems (C2): H = moderate to high (tool calls have side effects). Institutional systems (C3): H = high (organizational processes restructured around the system's capabilities are expensive to reverse). |
| **Δt** | max{0, (W + A) − T_commit}. When W = ∞ (the verification that temporal closure operators exist cannot succeed because they don't), Δt is structurally infinite for any deployment of a stateless system as a persistent agent. The verification of agency takes infinitely longer than the commitment to agency. |
| **σ** | Count of autonomy grants (C3 crossings) made without verification of temporal closure requirements. Operationally: deployments where a stateless system is treated as a stateful agent — where outputs are treated as commitments from a persistent identity without verifying that the system actually maintains persistent identity. |
| **σ_threshold** | Risk-proportional to the commitment level granted. Advisory use (C1): higher σ tolerable (each output is a suggestion, user provides the slow layer). Actuating use (C2): lower σ required (tool calls produce irreversible side effects). Institutional use (C3): σ_threshold should be ~0 (the system is making binding commitments on behalf of the organization). |

### σ Operationalization

σ counts *autonomy-verification gaps* — deployments where the commitment to treating the system as an agent was not preceded by verification that the system can sustain agency.

Paper [6] provides the verification criteria via three operators:

1. **Temporal Separation (Operator 1):** Does the system decompose into layers with distinct timescales? For transformers: no. Each invocation is stateless. There is no fast layer and slow layer — there is a single forward pass.

2. **Feedback Coupling (Operator 2):** Does the system maintain endogenous state z_t that evolves via dz_t/dt = F(z_t, u_t)? For transformers: no. The context window is read-only input, not endogenous state. When input stops (u_t = 0), the system does nothing — it awaits the next token. External scaffolding (conversation history, RAG, tool use) is not internal closure.

3. **Adaptive Controller (Operator 3):** Does the system monitor a coherence metric and modulate its own coupling when approaching critical thresholds? For transformers: no. There is no self-monitoring mechanism, no coherence metric, no adaptive regulation.

A system that fails all three operators and is deployed as a persistent agent has σ = 1 per deployment. The boundary crossing is the organizational commitment to treating a simulator as an agent.

### Failure Signature

The failure mode is the **simulator gap** — the difference between what the system appears to be (a coherent agent with persistent identity) and what it is (a stateless simulator producing contextually plausible outputs):

1. **Confident contradiction:** The system asserts X in one context and not-X in another, with no internal resistance. This is not a bug — it is the designed behavior of a system that has no endogenous state constraining its outputs across contexts. Each invocation is independent. Paper [10]'s temporal coherence invariant test demonstrates this empirically: all tested transformers fail.

2. **Citation laundering:** The system produces references that fit the rhetorical slot perfectly but do not exist or do not say what is claimed. Paper [10]'s epistemic grounding invariant test and the detector's empirical findings (Finding 2: namespace-dependent fabrication rates from 0% to 45%) demonstrate this. The system has no mechanism for references to constrain output — they decorate it.

3. **Persona instability:** A system deployed as a persistent assistant with a defined personality/role will drift across interactions. The "persona" is reconstructed from context each invocation, not maintained by endogenous state. Under perturbation (adversarial prompts, context manipulation), the persona dissolves — because it was never instantiated, only simulated.

4. **Infinite retry without learning:** Errors leave no residue. The system can make the same mistake identically across sessions because there is no learning mechanism at deployment time. Paper [10]'s irreversibility invariant test confirms: all tested systems show zero learning residue.

### Threshold Story

σ_threshold depends on the autonomy level:

- **C1 deployment (advisory):** The human provides the slow layer. The simulator gap is bridged by human judgment — each output is evaluated before action. σ can be high because the human is the verification mechanism. This is the safe deployment zone for current architectures.

- **C2 deployment (actuating):** The system executes tool calls, writes code, sends messages. Each actuation is a commitment with side effects. The simulator gap now has real consequences — the system makes commitments it cannot verify because it has no endogenous state to track what it has committed to. σ_threshold must be low, enforced by a governor (Paper [12]'s BLI pattern: language proposes, evidence commits).

- **C3 deployment (institutional):** The system is treated as a persistent agent — makes decisions on behalf of the organization, maintains "relationships" with users, operates autonomously. The simulator gap is maximally dangerous: the organization is treating simulator outputs as agent commitments. Every deployment at this level without temporal closure verification is a σ increment.

The current industry trajectory is moving deployments from C1 → C2 → C3 without corresponding verification of temporal closure requirements. σ is increasing at industry scale.

### Worked Example

**Autonomous customer service agent:**

- An organization deploys an LLM-based system as its primary customer service interface.
- The system is presented as "Alex, your account manager" — a persistent persona that "remembers" prior interactions (via conversation history retrieval) and "manages" the customer relationship.
- T_commit = organization announces deployment, trains customers to interact with "Alex," restructures support staffing. C3 crossed.
- W = time to verify temporal closure operators: (1) temporal separation — does "Alex" maintain distinct timescale layers? No, each interaction is a stateless inference. (2) Feedback coupling — does "Alex" have endogenous state? No, "memory" is retrieved context, not evolved state. (3) Adaptive controller — does "Alex" regulate its own coherence? No, there is no self-monitoring. W = ∞ (verification would reveal the operators are absent).
- A = time to reverse: retrain customers, rehire support staff, unwind "Alex" branding. Months to years.
- Δt = max{0, (W + A) − T_commit}: structurally infinite (W = ∞ because verification would reveal the operators are absent; T_commit = weeks to months for deployment).

Consequences: "Alex" confidently contradicts its own prior statements (no temporal coherence invariant). "Alex" promises a refund policy that doesn't exist (no epistemic grounding invariant). "Alex" loses its persona under adversarial input from a frustrated customer (no adaptive controller). Each of these failures is individually minor — but they accumulate because the organization's customers have been told they are interacting with a persistent agent, and their expectations (and the organization's legal exposure) are calibrated accordingly.

σ = 1 (one C3 deployment without temporal closure verification). But the downstream C1 failures (contradictions, fabrications, persona drift) accumulate at the rate of customer interactions — thousands per day.

### Connection to Papers [6, 10, 12]

Paper [6] establishes the three operators as necessary conditions for temporal closure and proves that stateless transformer architectures cannot satisfy them — this is a type mismatch, not a scaling limitation. Paper [10] provides empirical validation: all four invariants (temporal coherence, semantic conservation, epistemic grounding, irreversibility) are violated across all tested architectures regardless of scale.

Paper [12] provides the architectural solution: the BLI governor separates the simulator (LLM) from the agent (governed system). The LLM proposes; the governor verifies; the ledger commits. Temporal closure is achieved by the composite system (LLM + governor + ledger), not by the LLM alone. The fault domain framing adds: deploying the LLM alone as an agent is a C3 commitment without temporal closure verification — structurally identical to the general fault domain pattern of commitment outrunning verification.
