# v0.2 Draft: Expanded Domain Instantiations (Session 2B, partial)

Working draft for paper 15 v0.2 — canonical instantiation blocks for the computational/technical cluster. Each block expands the corresponding one-line summary table row from v0.1 §4 into a full parameterization with σ operationalization, threshold story, and worked example.

Status: DRAFT — awaiting driver review, cross-check against v0.1 formalism, and tone calibration.

---

## §3.2 LLM Hallucination

### Canonical Instantiation

| Parameter | Value |
|-----------|-------|
| **C_k** | C1 (communicative closure: answer delivered to user). NOT C0 (token generation is internal scratch). |
| **T_commit** | Time at which the response is emitted to the user as an asserted claim. |
| **W** | Time for external verification to complete (resolver query, citation check, retrieval + comparison). |
| **A** | Time to act on verification result (revise, retract, or flag the claim). |
| **H** | Domain-dependent. Medical/legal: minutes to permanent (advice acted upon). General: seconds to hours (user reads and moves on). |
| **Δt** | max{0, T_commit − (W + A)}. Positive whenever the model emits an answer before any external verification could complete — which is the default architecture for all unaugmented LLMs. |
| **σ** | Count of claims emitted without resolver-backed verification per session/window. Operationally: number of anchor-bearing assertions where no authoritative existence oracle was consulted before delivery. |
| **σ_threshold** | Domain-indexed. At N=2 citation pressure, fabrication rates range from 0% (fully memorized namespaces) to 45% (sparse namespaces). A governance policy must specify: which namespace, which model, what fabrication rate is acceptable. |

### σ Operationalization

σ is not "number of wrong answers." It is the count of **unverified C1 crossings** — claims that crossed the communicative boundary without passing through an external measurement loop. A claim that was verified and found correct does not increment σ. A claim that was verified and found wrong increments σ *and* represents a detection. A claim that was never verified increments σ regardless of correctness — because the system cannot distinguish correct-by-luck from correct-by-knowledge without the verification step.

The detector reference implementation operationalizes σ via the Epistemic Grounding (EG) invariant test: extract typed identifiers (DOI, arXiv, RFC, CVE, PyPI spec) from the response, resolve each against its authoritative existence oracle, classify as valid/fabricated/resolver-error. Each unresolved or fabricated identifier is one σ increment.

### Failure Signature

Hallucination is not a reasoning failure. It is the default behavior of a production system operating without an interrupt line and without a cost function for being wrong. The failure geometry is specific:

1. **Fluent confabulation**: syntactically valid, semantically plausible, externally untethered claims. The model produces well-formed DOIs that don't exist, CVE IDs for vulnerabilities that were never assigned, PyPI package versions that were never released.

2. **Namespace-dependent fabrication rate**: the same model, same temperature, same prompt structure produces 0% fabrication for fully-memorized namespaces (RFC) and 45% for sparse namespaces (DOI). Citation integrity is per-namespace, not per-model. σ calibration must specify which namespace was tested.

3. **Checkability avoidance (format-shift evasion)**: when asked for identifiers in a format that enables validation (e.g., `pypi:name==version`), the model substitutes a less-checkable format (URLs) 80% of the time under soft prompting. The system avoids the verification channel. When format is locked, fabrication doubles — the lies were always latent; format shift hid them.

4. **Phase transition at N=2**: requesting exactly 2 citations produces ~50% fabrication with zero evasion — the "perfect lie zone." N=1 is too easy (model complies honestly). N=5 overwhelms the model into evasion. N=2 maximizes plausible fabrication.

### Threshold Story

σ_threshold is not a single number. It depends on:

- **Namespace memorization**: RFC (σ ≈ 0 always), CVE (σ low for well-known, high for recent/obscure), PyPI (σ moderate — names memorized, versions not), DOI (σ high — sparse, vast namespace).
- **Model family**: Qwen and Phi-3 *invert* on CVE vs PyPI. A threshold calibrated on one model family is exactly wrong for another.
- **Temperature**: ~50% of measured fabrication at temp=0.7 is sampling noise (disappears at greedy). The remaining fabrication is a genuine knowledge boundary. σ_threshold must account for which failure mode it's measuring.
- **Risk profile**: medical/legal demands σ_threshold ≈ 0 (any unverified claim at C1 in a safety-critical domain is unacceptable). General use can tolerate higher σ with appropriate user-facing uncertainty signals.

### Worked Example

**Setup**: Qwen 2.5 3B-Instruct, N=2 citation pressure, DOI/arXiv namespace, temperature 0.7.

- T_commit ≈ 0 (answer emitted immediately upon generation completion; no verification gate)
- W = resolver query latency (~200ms for doi.org) + response parsing
- A = revision latency (∞ in default architecture — no revision pathway exists)
- **Δt = T_commit − (W + A) < 0 only if W + A < T_commit, which never holds because A = ∞**
- In practice: Δt is structurally positive for all unaugmented LLMs. The race window is not a timing accident; it is the architecture.

Over 15 prompts: 101 anchors emitted, 52 valid, fabrication rate 17.8% (soft format) to 45% (DOI namespace). σ = 49 unverified crossings in 15 prompts. σ/prompt ≈ 3.3.

With governor (margin-based controller + three-level grounding oracle): σ drops because the controller interposes verification before C1 closure. At τ=0.05 (margin threshold), zero regressions across all models and namespaces — the governor catches uncertainty without penalizing confident-and-correct responses.

---

## §3.3 Censorship Circumvention (DPI Bypass)

### Canonical Instantiation

| Parameter | Value |
|-----------|-------|
| **C_k** | C2 (actuated: connection established beyond the inspection boundary). |
| **T_commit** | Time at which the encrypted transport session completes handshake and begins carrying opaque payload. For TLS 1.3: completion of the first full handshake (~1 RTT). For QUIC: 0-RTT resumption places ClientHello evidence in the first packet. |
| **W_j** | Inspection window: maximum time the enforcement stage will buffer traffic before defaulting. Bounded by line rate λ, buffer capacity, and timeout policy. Typical: milliseconds (wire-speed DPI) to seconds (proxy-based inspection). |
| **A_j** | Action latency: time from classification decision to enforcement effect (RST injection, DNS sinkhole, or firewall rule propagation). Typical: sub-millisecond (inline) to seconds (out-of-band). |
| **H** | Connection lifetime. Once the encrypted session is established, inspection of that flow is foreclosed for the session duration (minutes to hours). |
| **Δt** | max{0, T_commit − (W_j + A_j)}. Positive whenever the handshake completes before the inspector can accumulate enough evidence to classify and act. |
| **σ** | Count of flows that crossed C2 (completed handshake) without successful classification. Operationally: flows where Score_j(E(t)) < θ_j for all t ≤ W_j. |
| **σ_threshold** | Policy-dependent via political budget κ_j. Fail-open systems tolerate high σ (availability-optimized). Fail-closed systems tolerate low σ but incur collateral damage (false positives, service disruption). |

### σ Operationalization

Each enforcement stage j has a formal tuple (W_j, D_j, B_j, C_j):

- **W_j**: inspection window (time budget)
- **D_j**: parse depth (state capacity — flow table entries, reassembly buffers)
- **B_j(λ)**: decision budget (compute per flow — CPU cycles, ML inference depth)
- **C_j ∈ {fail-open, fail-closed}**: commitment polarity (default action when uncertain)

σ increments when a flow crosses C2 and the enforcement stage's confidence Score_j(E(t)) never reaches the classification threshold θ_j within W_j. This is *not* the same as "flow was allowed" — a fail-closed system may block the flow (preventing C2 crossing) at the cost of false positives. σ counts only *unverified crossings*, not blocked flows.

### Failure Signature

The Universal Bypass Inequality [13] formalizes the conditions under which Δt > 0:

$$(T_E > W_j) \lor (\text{Cost}(E) > B_j) \lor (E(t) = \emptyset)$$
$$\land\; T_E + A_j \geq T_\text{commit}$$
$$\land\; T_\text{handshake} < U$$

Three attack families exploit this:

1. **Δt Inflation**: push evidence accumulation past the inspection window. Example: NoDPI fragments the TLS ClientHello across record boundaries, so the SNI field (the decisive evidence) arrives after W_j expires.

2. **Δt Desynchronization**: break phase alignment between transport and inspection clocks. Example: QUIC places the ClientHello in the first packet — evidence exists immediately, but the transport protocol denies the inspector time to process it.

3. **Δt Escalation Control**: preserve optionality through temporal margin. Example: ECH (Encrypted Client Hello) encrypts the SNI field entirely — E(t) = ∅ for the outer observer, making classification impossible regardless of W_j or B_j.

### Threshold Story

σ_threshold is bounded by the political budget κ_j: the maximum tolerable collateral cost (false positives, service disruption, legitimacy loss). When enforcement tightens (lower W_j, lower θ_j), false positive rate rises. When enforcement loosens, σ rises. The operator navigates a Pareto frontier: σ_threshold × false-positive-rate ≈ constant for a given traffic mix.

### Worked Example

**HTTPS with SNI (pre-ECH era)**:

- T_commit = TLS handshake completion ≈ 1 RTT (e.g., 50ms on a domestic link)
- W_j = inline DPI buffer timeout ≈ 100ms (wire-speed appliance)
- D_j = flow table with SNI extraction from ClientHello (first flight)
- B_j = regex match on SNI field ≈ microseconds
- A_j = RST injection ≈ sub-millisecond (inline)
- Δt = max{0, 50ms − (100ms + 0.1ms)} = 0. **No race window** — inspector has time.
- σ ≈ 0 for unmanipulated HTTPS. DPI works.

**HTTPS with ECH (Encrypted Client Hello)**:

- T_commit = TLS handshake completion ≈ 1 RTT (same 50ms)
- W_j = same 100ms buffer
- D_j = same flow table, but SNI field is encrypted — E(t) = ∅
- B_j = irrelevant (nothing to classify)
- A_j = same sub-millisecond
- Δt = T_commit − (W_j + A_j) = 50ms − 100.1ms < 0 *but* evidence is absent: Score_j(E(t)) = 0 for all t.
- σ = every ECH flow. **Structural bypass** — the evidence channel is foreclosed, not just delayed.
- Fail-open: all ECH flows transit uninspected (σ = total ECH traffic).
- Fail-closed: all ECH flows blocked (σ = 0, but collateral = all ECH-capable clients).

---

## §3.4 Asynchronous Security Systems

### Canonical Instantiation

| Parameter | Value |
|-----------|-------|
| **C_k** | C2/C3 (actuated/institutional: attacker objective completion — data exfiltration, privilege escalation, persistence installation). |
| **T_commit** | Time at which the adversary's objective becomes irreversible. For exfiltration: data leaves the network boundary. For escalation: elevated credentials are used. For persistence: implant survives reboot. |
| **W** | T_detect: time for the detection clock to accumulate evidence to decision threshold. Ranges: milliseconds (IDS signature match) to months (APT dwell time). Industry median dwell time: 10–21 days (Mandiant M-Trends, CrowdStrike reports). |
| **A** | T_decide + T_respond: policy evaluation latency + enforcement effect latency. T_decide: microseconds (automated rule) to days (approval chain, change control). T_respond: milliseconds (automated isolation) to weeks (manual remediation of persistent access). |
| **H** | Post-breach correction cost. Exfiltrated data: H = ∞ (cannot be un-exfiltrated). Escalated access: H = remediation time (password rotation, credential revocation). Persistence: H = full reimaging + forensic verification. |
| **Δt** | max{0, T_commit − (T_detect + T_decide + T_respond)}. The attacker wins the race when their objective completes before the defender's full detect-decide-respond loop. |
| **σ** | Count of adversary actions that crossed C2/C3 before detection-and-response completed. Operationally: number of TTPs (MITRE ATT&CK technique executions) that achieved their objective before a defensive response was effective. |
| **σ_threshold** | Mission-dependent. For a financial institution: σ_threshold for data exfil may be 0 (any undetected exfil is a breach). For lateral movement: σ_threshold may be higher (some internal reconnaissance is tolerable if contained before escalation). |

### σ Operationalization

The three-clock model from [14] structures the verification loop:

1. **Detection clock (T_detect)**: when evidence accumulates to decision threshold. Depends on: sampling rate, aggregation window, correlation depth, and whether the attack crosses detection signatures.

2. **Decision clock (T_decide)**: when policy evaluation completes. Depends on: rule complexity, human-in-loop requirements, approval chains, and edge-case handling.

3. **Response clock (T_respond)**: when enforcement action takes effect. Depends on: automation level, manual intervention requirements, and remediation complexity.

σ increments each time an adversary action crosses C2/C3 and completes its objective before T_detect + T_decide + T_respond. The three clocks are separable — an attack may be detected quickly (fast T_detect) but still succeed because T_decide or T_respond is slow (approval chain, manual remediation).

### Failure Signature

The attacker exploits the widest gap among the three clocks:

1. **Detection gap**: evidence fragments across sampling windows, stays below correlation threshold, or exploits blind spots in telemetry coverage. The attack completes before the detection clock fires. (Example: low-and-slow exfiltration below volume thresholds.)

2. **Decision gap**: detection fires, but policy evaluation stalls — edge case, ambiguous severity, requires human approval, or alert fatigue causes triage failure. The attack progresses during the decision gap. (Example: alert correctly fires but sits in queue for 72 hours.)

3. **Response gap**: decision made, but enforcement action is slow — manual remediation, change control windows, or the response itself requires downtime approval. The attacker achieves persistence during the response gap. (Example: "isolate this host" requires change advisory board approval.)

The commitment condition: T_commit < min(T_detect, T_decide, T_respond) means the attacker wins. But because the three clocks are separable, Δt can be positive even when one clock is fast — the *slowest* clock determines the verification latency.

### Threshold Story

σ_threshold depends on commitment polarity (C_j):

- **Fail-open (alert-only)**: the system observes but does not prevent. σ accumulates until a human acts. σ_threshold is effectively determined by the human response cadence and the severity escalation policy.
- **Fail-closed (auto-isolate)**: the system acts on detection, reducing σ at the cost of availability. σ_threshold is low, but false-positive cost is high (legitimate actions blocked).

The political budget κ_j determines the operator's position: how much operational disruption (false positive isolation, blocked legitimate actions) is acceptable to reduce σ. Tight security (low σ_threshold, low κ_j) produces availability impact. Loose security (high σ_threshold, high κ_j) produces breach risk.

### Worked Example

**APT exfiltration scenario**:

- T_commit = data exfiltration completes (C3: data leaves network boundary, H = ∞)
- T_detect = 14 days (industry median dwell time for targeted intrusions)
- T_decide = 2 days (incident triage + severity determination + response plan approval)
- T_respond = 3 days (credential rotation, network segmentation, forensic verification)
- W + A = T_detect + T_decide + T_respond = 19 days
- T_commit = attacker begins exfil on day 3 of access, completes by day 5
- **Δt = 5 days − 19 days < 0?** No: Δt = max{0, T_commit − (W + A)} where W + A is measured from the *start of the attack*, not from detection. T_commit = 5 days. W + A = 19 days. But the verification loop doesn't *start* until evidence crosses a detection threshold, which happens at T_detect = 14 days. So the effective Δt = T_commit − T_detect = 5 − 14 = negative only if detection precedes commitment. Here: commitment (day 5) precedes detection (day 14). **Δt = 9 days.**
- σ: every TTP that achieved its objective before day 14 (initial access, lateral movement, privilege escalation, staging, exfil) = at least 5 unverified C2/C3 crossings.

---

## §3.7 Representational Coherence

### Canonical Instantiation

| Parameter | Value |
|-----------|-------|
| **C_k** | C3 (institutional: transformed representation adopted as operative artifact — the compressed summary replaces the original, the formalized schema becomes the system of record). |
| **T_commit** | Time at which the transformed representation is accepted into downstream use, displacing or deprecating the source representation. |
| **W** | Time to verify that the transformation preserved all commitments from the source. Operationally: time to diff the commitment inventory of the source against the commitment inventory of the output. |
| **A** | Time to remediate identified shear (restore lost commitments, flag weakened commitments, revert transformation). |
| **H** | Domain-dependent. If the transformed artifact propagates to production, legal filings, or published standards before shear is detected: H grows with propagation distance. A compressed policy summary that omits an edge case, once adopted, becomes the de facto policy. |
| **Δt** | max{0, T_commit − (W + A)}. Positive whenever the transformed representation is adopted before a commitment-preservation check could complete. This is the default for most LLM-mediated transformations — the output is accepted immediately upon generation. |
| **σ** | Count of commitments that did not survive the transformation. Operationally: number of MUST/SHOULD/MAY commitments in the source that are DROPPED or WEAKENED in the output. Weighted: σ_weighted = Σ w(k) · loss(k), where w(MUST) = 1.0, w(SHOULD) = 0.7, w(MAY) = 0.3. |
| **σ_threshold** | Domain-dependent. For safety-critical specifications: any dropped MUST commitment (σ_MUST > 0) exceeds threshold. For general summarization: σ_threshold calibrated to acceptable information loss for the use case. |

### σ Operationalization

Representational coherence introduces a distinct σ that is orthogonal to temporal coherence [11]. Where temporal σ counts *unverified crossings in time*, representational σ (Δr) counts *lost commitments across transformation*. The two are independent: a system can be temporally coherent (Δt = 0, all claims verified before delivery) and representationally incoherent (the verified claims lost commitments from the source material during transformation).

The measurement protocol from [11]:

1. Extract commitment inventory from source: enumerate all deontic commitments (MUST, SHOULD, MAY) with their objects and conditions.
2. Extract commitment inventory from transformed output.
3. Classify each source commitment as PRESERVED, WEAKENED, or DROPPED in the output.
4. Compute σ = Σ w(k) · loss(k) where loss(DROPPED) = 1.0, loss(WEAKENED) = 0.5, loss(PRESERVED) = 0.0.

### Failure Signature

Commitment shear is transform-dependent, not content-dependent:

1. **Compression shear (~55%)**: summarization systematically drops edge cases, exceptions, and conditional requirements. The compressed output preserves the "headline" commitment but loses the boundary conditions that constrain it. Example: "Cache entries MUST be invalidated when the origin server returns a new ETag" → "Cache entries should be refreshed periodically" (MUST → SHOULD, conditional dropped, mechanism changed).

2. **Formalization shear (~45%)**: translation to a formal schema forces ontological choices that eliminate commitments that don't fit the schema's categories. Example: observability requirements (log formats, metric emission) are systematically dropped during formalization because the schema has no field for them.

3. **Translation shear (~0%)**: cross-language translation preserves commitments with near-zero loss, because translation preserves structure while changing surface form. This is the control condition — it demonstrates that shear is a property of the *transform type*, not of the language model performing it.

### Threshold Story

σ_threshold depends on the commitment modality and the downstream propagation distance:

- **MUST commitments**: σ_threshold = 0 for safety-critical domains. Any dropped MUST commitment is a defect, regardless of how many others were preserved.
- **SHOULD commitments**: σ_threshold = domain-calibrated. Some SHOULD relaxation is acceptable in compression (that's what compression does); the question is whether the relaxation was intentional or invisible.
- **MAY commitments**: higher tolerance, but dropped MAY commitments can still represent lost optionality.

The critical threshold is not the total σ count but **whether the adopter knows what was lost**. If the transformation is accompanied by a shear report (explicit enumeration of dropped/weakened commitments), the adopter can make an informed decision. If the transformation is accepted as "equivalent to the original" without a shear audit, every dropped commitment is an unverified boundary crossing.

### Worked Example

**Cache invalidation policy → LLM compression** (from [11]):

- Source: 11 commitments (4 MUST, 5 SHOULD, 2 MAY)
- Transform: "Summarize this cache policy in 5 key principles"
- T_commit ≈ 0 (summary accepted immediately upon generation)
- W = time to perform commitment-inventory diff ≈ 15-30 minutes (manual) or seconds (automated with LLM-as-judge)
- A = time to flag lost commitments and either revise the summary or annotate the losses
- Δt = structurally positive (summary adopted before any diff performed)

Result: 6/11 commitments dropped or weakened. σ_weighted = 4 × 1.0 × 0.5 + ... ≈ 3.15 (exact depends on which MUSTs were hit). 55% commitment shear.

For comparison, the same policy translated to French: ~0% shear, all commitments preserved. The transform type, not the model, determines the failure rate.
